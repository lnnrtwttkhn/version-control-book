---
title: "DataLad Nextcloud"
engine: knitr
execute:
  eval: false
title-block-style: none
author: ""
---

## Preparation

### Create an example dataset

First, we create an example dataset.
The steps below will download a small public neuroimaging dataset, and transform it into a DataLad dataset.
We will use the [MoAEpilot](https://www.fil.ion.ucl.ac.uk/spm/data/auditory/) dataset containing anatomical and functional images from a single participant, as well as some metadata.
This procedure is copied from the chapter ["Walk-through: Amazon S3 as a special remote"](https://handbook.datalad.org/en/latest/basics/101-139-s3.html#your-datalad-dataset) in the DataLad Handbook.

::: {.callout-note collapse="true" .column-margin}
### Details about the MoAEpilot dataset

The following text has been copied from the [MoAEpilot](https://www.fil.ion.ucl.ac.uk/spm/data/auditory/) website:

> This experiment was conducted by Geraint Rees under the direction of Karl Friston and the FIL methods group. The purpose was to explore equipment and techniques in the early days of our fMRI experience. As such it has not been formally written up, and is freely available for personal education and evaluation purposes.

> 96 acquisitions were made (RT=7s), in blocks of 6, giving 16 42s blocks. The condition for successive blocks alternated between rest and auditory stimulation, starting with rest. Auditory stimulation was with bi-syllabic words presented binaurally at a rate of 60 per minute. The functional data starts at acquisiton 4, image fM00223_004. Due to T1 effects it is advisable to discard the first few scans (there were no "dummy" lead-in scans).

> These whole brain BOLD/EPI images were acquired on a modified 2T Siemens MAGNETOM Vision system. Each acquisition consisted of 64 contiguous slices (64x64x64 3mm x 3mm x 3mm voxels). Acquisition took 6.05s, with the scan to scan repeat time (RT) set arbitrarily to 7s.
:::


First, move to the directory on your computer where you want to create the dataset.

```{bash}
cd <wherever-you-want-to-create-the-dataset> # <1>
```

Next, we create a new directory called `mydataset`, we download and extract the data, and then we move the extracted contents into our new directory.

```bash
mkdir mydataset # <1>
wget https://www.fil.ion.ucl.ac.uk/spm/download/data/MoAEpilot/MoAEpilot.bids.zip -O mydataset.zip # <2>
unzip mydataset.zip -d mydataset # <3>
rm mydataset.zip # <4>
cd mydataset # <5>
mv MoAEpilot/* . # <6>
rm -R MoAEpilot # <7>
```
1. Create a new directory (`mkdir`) for the datatset called `mydataset`
1. Download the dataset using `wget` and save the `.zip` file
1. Unzip the dataset and extract the contents into the `mydataset` directory
1. Remove the `.zip` file
1. `cd` into the folder of the dataset
1. Move (`mv`) the contents of the `MoAEpilot` into the root of the directory
1. Remove the (now empty) `MoAEpilot` folder

Now we can view the directory tree to see the dataset content:

```{bash}
tree
.
├── CHANGES
├── README
├── dataset_description.json
├── sub-01
│   ├── anat
│   │   └── sub-01_T1w.nii
│   └── func
│       ├── sub-01_task-auditory_bold.nii
│       └── sub-01_task-auditory_events.tsv
└── task-auditory_bold.json

4 directories, 7 files
```

We can turn our `neuro-data-s3` directory into a DataLad dataset with the `datalad create --force` command.
After that, we save the dataset with `datalad save`:

```{bash}
datalad create --force --description "neuro data to host via webdav"
datalad save -m "Add public data"
```

### Install DataLad NEXT

[DataLad NEXT](http://docs.datalad.org/projects/next/en/latest/index.html) is a DataLad extension that "can be thought of as a staging area for additional functionality, or for improved performance and user experience."

1. Install DataLad NEXT from [PyPi](https://pypi.org/project/datalad-next/) or [GitHub](https://github.com/datalad/datalad-next).

::: {.callout-tip collapse="true"}
### Tip: Use a virtual environment to install DataLad Next
```{bash}
# create and enter a new virtual environment (optional)
virtualenv --python=python3 ~/env/dl-next
. ~/env/dl-next/bin/activate
```
:::

```{bash}
python -m pip install datalad-next
```

2. Enable the extension for autoloading:

```{bash}
git config --global --add datalad.extensions.load next
```

## Walkthrough: Hosting DataLad datasets on Nextcloud

This walktrough demonstrates how to collaborate on DataLad datasets using the WebDAV-enabled cloud storage Nextcloud.
As an example, we use UHH-Cloud at University of Hamburg, which is based on Nextcloud.

> Nextcloud Hub is the industry-leading, fully open-source, on-premises content collaboration platform. Teams access, share and edit their documents, chat and participate in video calls and manage their mail and calendar and projects across mobile, desktop and web interfaces. -- https://nextcloud.com/about/

> *"Students have a storage quota of 100 gigabytes each, while all other users have a storage quota of 5 terabytes each."*
> -- translated from §2 (2) of the [terms of use for the UHH-Cloud service](https://www.rrz.uni-hamburg.de/services/kollaboration/uhhcloud/nutzungsbedingungen.html) (in German)

### Get the WebDAV address

1. Click on {{< fa gear >}} `Files settings` (bottom left)
1. Copy the WebDAV address, for example: `https://cloud.uni-hamburg.de/remote.php/dav/files/BBC5706%40uni-hamburg.de/`

### Create the WebDAV DataLad sibling

```{bash}
datalad create-sibling-webdav \
  --dataset . \
  --name uhhcloud \
  --mode filetree \
  'https://cloud.uni-hamburg.de/remote.php/dav/files/BBC5706%40uni-hamburg.de/neuro-data-s3'
```

You will be asked for a user name and a password:

```{bash}
User name and password are required for WebDAV access at https://cloud.uni-hamburg.de/remote.php/dav/files/BBC5706%40uni-hamburg.de/neuro-data-s3
```

### Create a new app password

1. Go to the {{< fa lock >}} [UHH-Cloud security settings](https://cloud.uni-hamburg.de/settings/user/security)
1. Select an app name (e.g., "DataLad") and click "Create new app passsword".
1. Save the username (e.g., `BBC5706@uni-hamburg.de`) and password securely (e.g., in a password manager).
You will need them repeatedly.

Enter the Nextcloud app username and password created in the previous step:

```{bash}
user: <your_username> # insert your username, e.g., BBC5706@uni-hamburg.de
password: # enter the app password
password (repeat): # enter the app password again
```

The following output confirms that the configuration of the Nextcloud DataLad sibling was successful:

```{bash}
create_sibling_webdav.storage(ok): . [uhhcloud-storage: https://cloud.uni-hamburg.de/remote.php/dav/files/BBC5706%40uni-hamburg.de/neuro-data-s3]
[INFO   ] Configure additional publication dependency on "uhhcloud-storage" 
create_sibling_webdav(ok): . [uhhcloud: datalad-annex::?type=webdav&encryption=none&exporttree=yes&url=https%3A//cloud.uni-hamburg.de/remote.php/dav/files/BBC5706%2540uni-hamburg.de/neuro-data-s3]
[WARNING] The entered credential will not be stored, a credential with the default name 'webdav-BBC5706@uni-hamburg.de-https://cloud.uni-hamburg.de/UHH-Cloud' already exists. Specify a credential name via the `credential` parameter  and/or configure a credential with the datalad-credentials command with a `realm=https://cloud.uni-hamburg.de/UHH-Cloud` property 
````

### Push the dataset to Nextcloud

```{bash}
datalad push --to uhhcloud
```

The following output confirms that the DataLad dataset has been successfully pushed to Nextcloud:

```{bash}
copy(ok): .datalad/.gitattributes (dataset)
copy(ok): .datalad/config (dataset)
copy(ok): .gitattributes (dataset)
copy(ok): CHANGES (dataset)
copy(ok): README (dataset)
copy(ok): dataset_description.json (dataset)
copy(ok): sub-01/anat/sub-01_T1w.nii (dataset)
copy(ok): sub-01/func/sub-01_task-auditory_bold.nii (dataset)
copy(ok): sub-01/func/sub-01_task-auditory_events.tsv (dataset)
copy(ok): task-auditory_bold.json (dataset)
publish(ok): . (dataset) [refs/heads/main->uhhcloud:refs/heads/main [new branch]]
publish(ok): . (dataset) [refs/heads/git-annex->uhhcloud:refs/heads/git-annex [new branch]]
action summary:
  copy (ok: 10)
  publish (ok: 2)
```

We can now view the [files on UHH-Cloud](https://cloud.uni-hamburg.de/apps/files/) and inspect them through the web browser.

![](../static/images/nextcloud_datalad_push.png)
Note that on the cloud storage portal, the dataset content looks like any other upload but it remains a complete datatset that can be cloned as if it would be hosted on GitLab or GitHub (for details, see the next chapter).

### Clone dataset

```{bash}
cd .. # <1>
datalad clone \
  'webdavs://cloud.uni-hamburg.de/remote.php/dav/files/BBC5706%40uni-hamburg.de/neuro-data-s3' \
  neuro-data-s3-clone # <2>
```
1. `cd` out of the current dataset. You can also chose any other target directory on your system.
2. Make sure to replace `https` with `webdavs`!

Note one important aspect of this step:
We again use the WebDAV address to clone the dataset but replace `https` with `webdavs`.

To complete cloning the dataset, we need to provide the access credentials again:

```{bash}
A credential is required for access
user: # <1>
password:
password (repeat):
```
1. Enter your app password user address (e.g., `BBC5706@uni-hamburg.de`)

```{bash}
[INFO   ] Remote origin uses a protocol not supported by git-annex; setting annex-ignore
[INFO   ] access to 1 dataset sibling uhhcloud-storage not auto-enabled, enable with:
| 		datalad siblings -d "/tmp/neuro-data-s3-clone" enable -s uhhcloud-storage
install(ok): /tmp/neuro-data-s3-clone (dataset)
```

We use the suggested command 

```{bash}
datalad siblings -d "/tmp/neuro-data-s3-clone" enable -s uhhcloud-storage
```

```{bash}
CommandError: 'git -c diff.ignoreSubmodules=none annex enableremote uhhcloud-storage -c annex.dotfiles=true' failed with exitcode 1 under /tmp/neuro-data-s3-clone
enableremote uhhcloud-storage 

failed
  Set both WEBDAV_USERNAME and WEBDAV_PASSWORD to use webdav
git-annex: Need to configure webdav username and password.
CallStack (from HasCallStack):
  error, called at ./Remote/WebDAV.hs:324:21 in main:Remote.WebDAV
```

Set `WEBDAV_USERNAME` and `WEBDAV_PASSWORD`

```{bash}
DATALAD_CREDENTIAL_WEBDAV_USERNAME=BBC5706@uni-hamburg.de # <1>
DATALAD_CREDENTIAL_WEBDAV_PASSWORD=gUdrc-ZX8sT-PqJLf-YRmS4-hKMPW # <2>
```
1. Enter your app user address (e.g., `BBC5706@uni-hamburg.de`)
2. Enter your app user password (e.g., `gUdrc-ZX8sT-PqJLf-YRmS4-hKMPW`)

```{bash}
cd tmp
WEBDAV_USERNAME=BBC5706@uni-hamburg.de \
  WEBDAV_PASSWORD=gUdrc-ZX8sT-PqJLf-YRmS4-hKMPW \
  git annex enableremote uhhcloud-storage
```

The output confirms that initializing the `uhhcloud-storage` remote was successful:

```{bash}
enableremote uhhcloud-storage (testing WebDAV server...) ok
(recording state in git...)
```

We can now run `datalad get .` to retrieve all contents of the dataset:

```{bash}
datalad get . 
```

The output confirms that all contents were successfully retrieved from the `uhhcloud-storage` sibling: 

```{bash}
get(ok): CHANGES (file) [from uhhcloud-storage...]
get(ok): README (file) [from uhhcloud-storage...]
get(ok): dataset_description.json (file) [from uhhcloud-storage...]
get(ok): sub-01/anat/sub-01_T1w.nii (file) [from uhhcloud-storage...]
get(ok): sub-01/func/sub-01_task-auditory_bold.nii (file) [from uhhcloud-storage...]
get(ok): sub-01/func/sub-01_task-auditory_events.tsv (file) [from uhhcloud-storage...]
get(ok): task-auditory_bold.json (file) [from uhhcloud-storage...]
action summary:
  get (ok: 7)
```

Now the dataset is fully ready for local use and complete with its entire history.

### Link sharing

Nextcloud allows to create a public and private links that can be used to share the dataset.

- :sparkles: **Multiple links:** Use of multiple share links with varying permissions and individual labels
- :sparkles: **Custom permissions:** Flexibly combine read, upload, edit or delete permissions
- :sparkles: **Options:** Hide download, password protect, set expiration date, note to recipient

### Links

- [YouTube video: "Deposit and retrieve DataLad datasets with WebDAV services"](https://www.youtube.com/watch?v=XkcwpqPQHQY)
- [UHH-Cloud](https://cloud.uni-hamburg.de/) at the University of Hamburg, Germany
-  [Details on UHH-Cloud](https://www.rrz.uni-hamburg.de/services/kollaboration/uhhcloud.html) (in German)
- ["Accessing Nextcloud files using WebDAV" - Nextcloud user manual](https://docs.nextcloud.com/server/25/user_manual/en/files/access_webdav.html)
- [DataLad NEXT extension](https://github.com/datalad/datalad-next)
- [`datalad create-sibling-webdav` documentation](http://docs.datalad.org/projects/next/en/latest/generated/man/datalad-create-sibling-webdav.html)
- [WebDAV git annex](https://git-annex.branchable.com/special_remotes/webdav/)

## Walkthough: Hosting DataLad datasets on ownCloud

> 50 GByte default storage space per user; flexible increase possible upon request 

### Get the WebDAV address

1. Click on {{< fa gear >}} `Settings` (bottom left)
1. Copy the WebDAV address, for example: `https://owncloud.gwdg.de/remote.php/nonshib-webdav/`

```{bash}
datalad create-sibling-webdav \
  --dataset . \
  --name owncloud-gwdg \
  --mode filetree \
  'https://owncloud.gwdg.de/remote.php/nonshib-webdav/<dataset-name>'
```

Replace `<dataset-name>` with the name of your dataset, i.e., the name of your dataset folder.
In this example, we replace `<dataset-name>` with `neuro-data-s3`.
The complete command for your example hence looks like this:

```{bash}
datalad create-sibling-webdav \
  --dataset . \
  --name owncloud-gwdg \
  --mode filetree \
  'https://owncloud.gwdg.de/remote.php/nonshib-webdav/neuro-data-s3'
```

You will be asked to provide your ownCloud account credentials:

```{bash}
user: # <1>
password: # <2>
password (repeat): # <3>
```
1. Enter the email address of your ownCloud account.
2. Enter the password of your ownCloud account.
3. Repeat the password of your ownCloud account.

```{bash}
create_sibling_webdav.storage(ok): . [owncloud-gwdg-storage: https://owncloud.gwdg.de/remote.php/nonshib-webdav/neuro-data-s3]
[INFO   ] Configure additional publication dependency on "owncloud-gwdg-storage" 
create_sibling_webdav(ok): . [owncloud-gwdg: datalad-annex::?type=webdav&encryption=none&exporttree=yes&url=https%3A//owncloud.gwdg.de/remote.php/nonshib-webdav/neuro-data-s3]
[WARNING] The entered credential will not be stored, a credential with the default name 'webdav-wittkuhn@mpib-berlin.mpg.de-https://owncloud.gwdg.de/ownCloud' already exists. Specify a credential name via the `credential` parameter  and/or configure a credential with the datalad-credentials command with a `realm=https://owncloud.gwdg.de/ownCloud` property 
```

Finally, we can push the dataset to ownCloud:

```{bash}
datalad push --to owncloud-gwdg # <1>
```
1. Use `datalad push` to push the dataset contents to ownCloud.
For details on `datalad push`, see the [command line reference](http://docs.datalad.org/en/stable/generated/man/datalad-push.html) and [this chapter](https://handbook.datalad.org/en/latest/basics/101-141-push.html) in the DataLad Handbook.

We can now view the [files on ownCloud](https://owncloud.gwdg.de/index.php/apps/files/) and inspect them through the web browser:

![](../static/images/owncloud_datalad_push.png)

### Public sharing

Create a public share link.

### Interacting with non datalad users.


### References

- https://www.gwdg.de/storage-services/gwdg-owncloud
